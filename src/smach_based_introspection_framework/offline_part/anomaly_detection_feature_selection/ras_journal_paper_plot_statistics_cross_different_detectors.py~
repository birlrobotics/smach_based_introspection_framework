from smach_based_introspection_framework._constant import (
    datasets_of_filtering_schemes_folder, folder_time_fmt
)
import glob
import os
import coloredlogs, logging
from smach_based_introspection_framework.online_part.anomaly_detector import Detectors 
from sklearn.externals import joblib
import pandas as pd
import numpy as np
import pickle
import datetime
import re
import ipdb
import matplotlib.pyplot as plt
import matplotlib
matplotlib.rcParams['font.size'] = 8

def get_first_anomaly_signal_time(detector, model, timeseries_mat, ts):
    loglik_and_threshold = []
    first_t = None
    for idx, t in enumerate(ts):
        now_skill, anomaly_detected, metric, threshold = detector.add_one_smaple_and_identify_skill_and_detect_anomaly(
                            np.array(timeseries_mat[idx]).reshape(1,-1), now_skill=1)
        loglik_and_threshold.append([metric, threshold])
        if anomaly_detected and first_t is None:
            first_t = t
    return first_t, loglik_and_threshold

def run():
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    consoleHandler = logging.StreamHandler()
    consoleHandler.setLevel(logging.INFO)
    logger.addHandler(consoleHandler)
    
    model_folders = glob.glob(os.path.join(
        datasets_of_filtering_schemes_folder,
        'introspection_models',
        'No.* filtering scheme', 
        'skill *',
    ))
    if len(model_folders) == 0:
        logger.error('Without any introspection model')
        
    for model_folder in model_folders:
        logger.info(os.path.realpath(model_folder))
        model = joblib.load(os.path.join(model_folder, 'introspection_model'))

        avaliable_anomaly_detectors=[
                Detectors.DetectorBasedOnLogLikByHiddenState(
                {1: model['hmm_model']}, 
                {1: model['zhat_loglik_threshold_by_max_min_dict']},) #zhat_loglik_threshold_by_mean_std_dict
                ,
                Detectors.DetectorBasedOnGradientOfLoglikCurve(
                    {1: model['hmm_model']}, 
                    {1: model['threshold_for_introspection']},)
            ]
            
        path_postfix = os.path.relpath(model_folder,
                                   os.path.join(datasets_of_filtering_schemes_folder, 'introspection_models'))
        succ_folder = os.path.join(datasets_of_filtering_schemes_folder,
                                   path_postfix).replace(os.sep+"skill", os.sep+"successful_skills"+os.sep+"skill")
        unsucc_folder = os.path.join(datasets_of_filtering_schemes_folder,
                                   path_postfix).replace(os.sep+"skill", os.sep+"unsuccessful_skills"+os.sep+"skill")
        
        output_dir = os.path.join(
            datasets_of_filtering_schemes_folder,
            'ras_journal_paper_plots',
            path_postfix,)
        if not os.path.isdir(output_dir):
            os.makedirs(output_dir)

        stat_df = pd.DataFrame(columns=['detector','sample_name', 'anomaly_type', 'TP', 'TN', 'FP', 'FN'])            
            
        succ_csvs   = glob.glob(os.path.join(succ_folder, '*', '*.csv'))
        if len(succ_csvs) == 0:
            logger.error('without the successful recordings of %s for testing'%path_postfix)
            continue
        for i, csv in enumerate(succ_csvs):
            logger.info(csv)
            df = pd.read_csv(csv, sep=',')
            for detector in avaliable_anomaly_detectors:
                first_anomaly_t, loglik_and_threshold = get_first_anomaly_signal_time(detector, model,
                                                                                      df.values[:, 1:], df.values[:, 0].reshape(-1))
                stat = {}
                if first_anomaly_t is not None:
                    stat['FP'] = 1
                else:
                    stat['TN'] = 1
                stat.update({"detector":detector.__name__, "sample_name": os.path.basename(csv),'anomaly_type': 'success'})
                stat_df = stat_df.append(stat, ignore_index=True)
            logger.warning("Finish testing:%s"%csv)

        unsucc_csvs = glob.glob(os.path.join(unsucc_folder, '*', '*.csv'))
        if len(unsucc_csvs) == 0:
            logger.error('without the unsuccessful recordings of %s for testing'%path_postfix)
            continue
        for i, csv in enumerate(unsucc_csvs):
            logger.info(csv)
            df = pd.read_csv(csv, sep=',')
            anomaly_label_and_signal_time = pickle.load(open(os.path.join(
                os.path.dirname(csv), 'anomaly_label_and_signal_time.pkl'), 'r'))
            anomaly_type = anomaly_label_and_signal_time[0]
            anomaly_t_by_human = anomaly_label_and_signal_time[1].to_sec()
            for detector in avaliable_anomaly_detectors:
                first_anomaly_t, loglik_and_threshold = get_first_anomaly_signal_time(detector, model,
                                                                                      df.values[:, 1:], df.values[:, 0].reshape(-1))
                stat = {}
                if first_anomaly_t is None:
                    stat['FN'] = 1
                else:
                    if anomaly_type != 'no_object':
                        t_diff = abs(anomaly_t_by_human - first_anomaly_t)
                        if t_diff > 1:
                            stat['FP'] = 1
                        else:
                            stat['TP'] = 1
                    else:
                        stat['TP'] = 1
                stat.update({"detector":detector.__name__, "sample_name": os.path.basename(csv), 'anomaly_type': anomaly_type})
                stat_df = stat_df.append(stat, ignore_index=True)
            logger.warning("Finish testing:%s"%csv)
        stat_file = os.path.join(output_dir, os.path.basename(output_dir)+' stat.csv')
        if os.path.isfile(stat_file):
            logger.warning("Stat file already exists, rename the existing file")
            postfix = '_at_%s'%datetime.datetime.now().strftime(folder_time_fmt)
            os.rename(stat_file, os.path.join(output_dir, os.path.basename(output_dir)+ ' stat.csv.%s'%postfix))
        stat_df.to_csv(stat_file)
        
        prog = re.compile(r'No.(\d+) filtering scheme%sskill (\d+)'%os.sep)        
        m = prog.search(model_folder)
        if not m:
            raise Exception("Fail to extract scheme no and skill no from %s"%csv)
        scheme_no = m.group(1)
        skill_no = m.group(2)
        for detector in avaliable_anomaly_detectors:
            df_ = stat_df.loc[stat_df['detector']==detector.__name__]
            ipdb.set_trace()
            df = append_metrics(df_.groupby(level=[0]).sum())
            
            
def append_metrics(df_):
    df = df_.copy()
    df['precision'] = df['TP']/(df['TP']+df['FP']) 
    df['recall'] = df['TP']/(df['TP']+df['FN']) 
    df['F1score'] = 2*df['TP']/(2*df['TP']+df['FP']+df['FN'])
    df['accuracy'] = (df['TP']+df['TN'])/(df['TP']+df['TN']+df['FP']+df['FN']) 
    return df
        



        
if __name__ == '__main__':
    run()
